{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNISTのデータセットを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_supervised=True：入力のデータと正解データをタプルにして返す\n",
    "# with_info=True：データセットの情報を返す (mnist_info)\n",
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練用とテスト用のデータセットを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 今回は訓練用データをデータセットの10%とする\u001b[39;00m\n\u001b[1;32m      4\u001b[0m num_validation_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m mnist_info\u001b[38;5;241m.\u001b[39msplits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_examples\n\u001b[0;32m----> 5\u001b[0m num_validation_samples \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(num_validation_samples, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# データに型付けする\u001b[39;00m\n\u001b[1;32m      9\u001b[0m num_test_samples \u001b[38;5;241m=\u001b[39m mnist_info\u001b[38;5;241m.\u001b[39msplits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_examples\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "mnist_train, mnist_test = mnist_dataset[\"train\"], mnist_dataset[\"test\"]\n",
    "\n",
    "# 今回は訓練用データをデータセットの10%とする\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "\n",
    "# データに型付けする\n",
    "num_test_samples = mnist_info.splits[\"test\"].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "print(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準化する (具体的には0 ~ 255の値を0 ~ 1の値に変換する)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    \"\"\"\n",
    "    画像データのピクセル値を0~1にスケーリングする\n",
    "    \"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データシャッフル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 23:23:00.094351: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(\n",
    "    BUFFER_SIZE\n",
    ")\n",
    "#  take()：指定した数の要素を取得する\n",
    "validation_data = shuffled_train_and_validation_data.take(int(num_validation_samples))\n",
    "# skip()：指定した数の要素をスキップする,\n",
    "train_data = shuffled_train_and_validation_data.skip(int(num_validation_samples))\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# データをバッチサイズごとに分割する\n",
    "# バックプロパゲーションの処理は重いため\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# バリデーションとテストデータは一度に読み込む、フォワードプロパゲーションのみ行うため\n",
    "validation_data = validation_data.batch(int(num_validation_samples))\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# tf.keras.Sequential()：モデルを作成する\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        # tf.keras.layers.Flatten()：入力データを1次元に変換する\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        # tf.keras.layers.Dense()：全結合層を作成する\n",
    "        # units：出力の次元数\n",
    "        # activation：活性化関数\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(output_size, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化アルゴリズムと損失関数を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.optimizers.Adam()：Adamアルゴリズムを使用してモデルを最適化する\n",
    "# loss：損失関数\n",
    "# sparse_categorical_crossentropyとは：https://qiita.com/omiita/items/0b7d4c0c6d5d4c4b0b0e\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 3s - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.0248 - val_accuracy: 0.9933 - 3s/epoch - 5ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.0271 - val_accuracy: 0.9918 - 3s/epoch - 5ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.0205 - val_accuracy: 0.9952 - 3s/epoch - 5ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0094 - accuracy: 0.9975 - val_loss: 0.0188 - val_accuracy: 0.9942 - 3s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 2s - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.0427 - val_accuracy: 0.9868 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29a4de7d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 繰り返した回数\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "VALIDATION_STEPS = num_validation_samples\n",
    "\n",
    "# モデルを訓練する\n",
    "model.fit(train_data, epochs=NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), validation_steps=VALIDATION_STEPS, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 224ms/step - loss: 0.1475 - accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と精度を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.15. Test accuracy: 96.97%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {0:.2f}. Test accuracy: {1:.2f}%\".format(test_loss, test_accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
